# Explorando o ViÃ©s em Deep Learning: ClassificaÃ§Ã£o de GÃªnero e RaÃ§a com Datasets Balanceados e Enviesados

### ğŸ¯ Objetivo do Experimento
Comparar o impacto do viÃ©s nos datasets FairFace (balanceado) e CelebA (enviesado), treinando um modelo de IA (ConvNeXt-Tiny) para prever raÃ§a e/ou gÃªnero, e avaliando justiÃ§a e desempenho por grupo.

### ğŸ” PreparaÃ§Ã£o dos Dados
- Padronizou ambos os datasets (FairFace e CelebA).
- Criou atributo raÃ§a no CelebA (que nÃ£o tinha originalmente).
- Dividiu os dois datasets igualmente:
- 85 mil imagens para treinamento
- 10 mil imagens para validaÃ§Ã£o
- Salvou os rÃ³tulos em arquivos CSV, sem usar subpastas por classe.

Foi realizada a padronizaÃ§Ã£o dos datasets FairFace e CelebA por meio de tÃ©cnicas de dataset downsampling, schema alignment e data preprocessing, incluindo limpeza de atributos, seleÃ§Ã£o de instÃ¢ncias e reformataÃ§Ã£o de caminhos de arquivos. Para lidar com a ausÃªncia do atributo racial no CelebA, foi utilizado pseudo-labeling via modelo prÃ©-treinado (FairFace), com posterior unificaÃ§Ã£o de estrutura de metadados em arquivos CSV padronizados.

### ğŸ§  Modelo Escolhido

Vai usar o ConvNeXt-Tiny, que Ã© moderno, poderoso e sensÃ­vel a padrÃµes sutis â€” ideal para detectar viÃ©s.
AdaptarÃ¡ a Ãºltima camada para prever raÃ§a ou gÃªnero (multi tarefa).

### âœ… Plano de Treinamento e AvaliaÃ§Ã£o
A) Treinar um modelos com dois datasets:
- CelebA (GÃªnero e RaÃ§a desequilibrados)
- FairFace (GÃªnero e RaÃ§a equilibrados)

B) Avaliar cada modelo:
- No seu prÃ³prio conjunto de validaÃ§Ã£o (padrÃ£o)
- No conjunto de validaÃ§Ã£o do outro dataset (teste cruzado)
- Comparar desempenho por grupo, como: Mulheres negras, homens brancos, etc.

### ğŸ“Š MÃ©tricas e AnÃ¡lises
- AcurÃ¡cia, Precision, Recall, F1 por grupo (raÃ§a Ã— gÃªnero)

### ğŸ› ï¸ Ferramentas Ãºteis
- PyTorch para treino
- Pandas

